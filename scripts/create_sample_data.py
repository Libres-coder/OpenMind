"""
åˆ›å»ºç¤ºä¾‹è®­ç»ƒæ•°æ®
ç”¨äºå¿«é€ŸéªŒè¯è®­ç»ƒpipeline
"""
import json
import os
from pathlib import Path

def create_sample_pretrain_data():
    """åˆ›å»ºé¢„è®­ç»ƒç¤ºä¾‹æ•°æ®"""
    output_dir = Path("data/sample/pretrain")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print("åˆ›å»ºé¢„è®­ç»ƒç¤ºä¾‹æ•°æ®...")
    
    samples = []
    for i in range(100):
        sample = {
            "text": f"è¿™æ˜¯ç¬¬{i}ä¸ªè®­ç»ƒæ ·æœ¬ã€‚è¿™ä¸ªæ ·æœ¬åŒ…å«äº†å¤šæ¨¡æ€å†…å®¹çš„æè¿°æ–‡æœ¬ï¼Œç”¨äºéªŒè¯æ¨¡å‹çš„åŸºæœ¬è®­ç»ƒæµç¨‹ã€‚",
            "metadata": {
                "source": "sample",
                "id": i,
                "quality": 1.0
            }
        }
        samples.append(sample)
    
    output_file = output_dir / "train.jsonl"
    with open(output_file, 'w', encoding='utf-8') as f:
        for sample in samples:
            f.write(json.dumps(sample, ensure_ascii=False) + '\n')
    
    print(f"âœ… åˆ›å»ºäº† {len(samples)} ä¸ªé¢„è®­ç»ƒæ ·æœ¬")
    print(f"   æ–‡ä»¶ä½ç½®: {output_file}")
    return output_file

def create_sample_sft_data():
    """åˆ›å»ºæŒ‡ä»¤å¾®è°ƒç¤ºä¾‹æ•°æ®"""
    output_dir = Path("data/sample/sft")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print("\nåˆ›å»ºæŒ‡ä»¤å¾®è°ƒç¤ºä¾‹æ•°æ®...")
    
    conversation_templates = [
        {
            "conversations": [
                {"from": "user", "value": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"},
                {"from": "assistant", "value": "ä½ å¥½ï¼æˆ‘æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€AIåŠ©æ‰‹ï¼Œå¯ä»¥å¤„ç†æ–‡æœ¬ã€å›¾åƒå’ŒéŸ³é¢‘ç­‰å¤šç§è¾“å…¥ï¼Œå¸®åŠ©ä½ å®Œæˆå„ç§ä»»åŠ¡ã€‚"}
            ]
        },
        {
            "conversations": [
                {"from": "user", "value": "è¯·è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"},
                {"from": "assistant", "value": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ æ¨¡å¼å’Œè§„å¾‹ï¼Œå¹¶åŸºäºè¿™äº›å­¦ä¹ åšå‡ºé¢„æµ‹æˆ–å†³ç­–ï¼Œè€Œæ— éœ€æ˜ç¡®ç¼–ç¨‹æ¯ä¸€ä¸ªæ­¥éª¤ã€‚"}
            ]
        },
        {
            "conversations": [
                {"from": "user", "value": "å¦‚ä½•æé«˜æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Ÿ"},
                {"from": "assistant", "value": "æé«˜æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ–¹æ³•åŒ…æ‹¬ï¼š1) ä½¿ç”¨æ€ç»´é“¾(Chain-of-Thought)æç¤ºï¼›2) å¢åŠ æ¨ç†ä»»åŠ¡çš„è®­ç»ƒæ•°æ®ï¼›3) é‡‡ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œä¼˜åŒ–ï¼›4) å¼•å…¥è‡ªæˆ‘éªŒè¯æœºåˆ¶ã€‚"}
            ]
        },
        {
            "conversations": [
                {"from": "user", "value": "å¤šæ¨¡æ€æ¨¡å‹æœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ"},
                {"from": "assistant", "value": "å¤šæ¨¡æ€æ¨¡å‹çš„ä¼˜åŠ¿åŒ…æ‹¬ï¼š1) èƒ½å¤ŸåŒæ—¶å¤„ç†å¤šç§ç±»å‹çš„è¾“å…¥ï¼›2) å¯ä»¥ç†è§£ä¸åŒæ¨¡æ€ä¹‹é—´çš„å…³è”ï¼›3) åœ¨å¤æ‚ä»»åŠ¡ä¸Šè¡¨ç°æ›´å¥½ï¼›4) æ›´æ¥è¿‘äººç±»çš„æ„ŸçŸ¥å’Œç†è§£æ–¹å¼ã€‚"}
            ]
        },
        {
            "conversations": [
                {"from": "user", "value": "è¯·ç”¨ç®€å•çš„è¯è§£é‡Šä»€ä¹ˆæ˜¯ç¥ç»ç½‘ç»œã€‚"},
                {"from": "assistant", "value": "ç¥ç»ç½‘ç»œæ˜¯ä¸€ç§æ¨¡ä»¿äººè„‘å·¥ä½œæ–¹å¼çš„è®¡ç®—æ¨¡å‹ã€‚å®ƒç”±è®¸å¤šç›¸äº’è¿æ¥çš„\"ç¥ç»å…ƒ\"ç»„æˆï¼Œè¿™äº›ç¥ç»å…ƒå¯ä»¥å­¦ä¹ è¯†åˆ«æ•°æ®ä¸­çš„æ¨¡å¼ï¼Œå°±åƒäººè„‘å­¦ä¹ è¯†åˆ«ç‰©ä½“å’Œæ¦‚å¿µä¸€æ ·ã€‚"}
            ]
        }
    ]
    
    samples = conversation_templates * 20
    
    output_file = output_dir / "train.jsonl"
    with open(output_file, 'w', encoding='utf-8') as f:
        for sample in samples:
            f.write(json.dumps(sample, ensure_ascii=False) + '\n')
    
    print(f"âœ… åˆ›å»ºäº† {len(samples)} ä¸ªSFTæ ·æœ¬")
    print(f"   æ–‡ä»¶ä½ç½®: {output_file}")
    return output_file

def create_sample_eval_data():
    """åˆ›å»ºè¯„ä¼°ç¤ºä¾‹æ•°æ®"""
    output_dir = Path("data/sample/eval")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print("\nåˆ›å»ºè¯„ä¼°ç¤ºä¾‹æ•°æ®...")
    
    samples = [
        {
            "conversations": [
                {"from": "user", "value": "1+1ç­‰äºå‡ ï¼Ÿ"},
                {"from": "assistant", "value": "2"}
            ]
        },
        {
            "conversations": [
                {"from": "user", "value": "è¯·åˆ—ä¸¾ä¸‰ç§å¸¸è§çš„ç¼–ç¨‹è¯­è¨€ã€‚"},
                {"from": "assistant", "value": "Pythonã€JavaScriptã€Java"}
            ]
        }
    ] * 10
    
    output_file = output_dir / "eval.jsonl"
    with open(output_file, 'w', encoding='utf-8') as f:
        for sample in samples:
            f.write(json.dumps(sample, ensure_ascii=False) + '\n')
    
    print(f"âœ… åˆ›å»ºäº† {len(samples)} ä¸ªè¯„ä¼°æ ·æœ¬")
    print(f"   æ–‡ä»¶ä½ç½®: {output_file}")
    return output_file

def main():
    print("="*60)
    print("ğŸ¨ åˆ›å»ºç¤ºä¾‹è®­ç»ƒæ•°æ®")
    print("="*60)
    
    create_sample_pretrain_data()
    create_sample_sft_data()
    create_sample_eval_data()
    
    print("\n" + "="*60)
    print("âœ… æ‰€æœ‰ç¤ºä¾‹æ•°æ®åˆ›å»ºå®Œæˆï¼")
    print("="*60)
    
    print("\nğŸ“ æ•°æ®æ–‡ä»¶ä½ç½®:")
    print("  é¢„è®­ç»ƒ: data/sample/pretrain/train.jsonl")
    print("  æŒ‡ä»¤å¾®è°ƒ: data/sample/sft/train.jsonl")
    print("  è¯„ä¼°: data/sample/eval/eval.jsonl")
    
    print("\nğŸ“ ä¸‹ä¸€æ­¥:")
    print("  è¿è¡Œ: python scripts/test_data_pipeline.py")

if __name__ == "__main__":
    main()
