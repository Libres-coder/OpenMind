"""
ç”Ÿäº§çº§è®­ç»ƒå™¨ - ç¨³å®šæ€§å’Œç›‘æ§å¢å¼º
åŒ…å«ï¼šæ¢¯åº¦ç›‘æ§ã€Lossæ£€æµ‹ã€è‡ªåŠ¨æ¢å¤ã€æ··åˆç²¾åº¦è®­ç»ƒ
"""
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from transformers import get_cosine_schedule_with_warmup
from tqdm import tqdm
import logging
from pathlib import Path
import json
from typing import Dict, Optional
import numpy as np

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ProductionTrainer:
    """å·¥ç¨‹çº§è®­ç»ƒå™¨ - ç¨³å®šæ€§å’Œç›‘æ§å¢å¼º"""
    
    def __init__(self, model, config: Dict):
        self.model = model
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # ä¼˜åŒ–å™¨
        self.optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=config.get('learning_rate', 2e-5),
            weight_decay=config.get('weight_decay', 0.01),
            betas=(0.9, 0.95)
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦
        self.scheduler = get_cosine_schedule_with_warmup(
            self.optimizer,
            num_warmup_steps=config.get('warmup_steps', 100),
            num_training_steps=config.get('total_steps', 10000)
        )
        
        # æ··åˆç²¾åº¦
        self.use_amp = config.get('use_amp', True)
        self.scaler = torch.cuda.amp.GradScaler() if self.use_amp else None
        
        # ç›‘æ§æŒ‡æ ‡
        self.loss_history = []
        self.best_loss = float('inf')
        self.patience_counter = 0
        self.global_step = 0
        
        # æ¢¯åº¦ç´¯ç§¯
        self.gradient_accumulation_steps = config.get('gradient_accumulation', 4)
        
        # è¾“å‡ºç›®å½•
        self.output_dir = Path(config.get('output_dir', 'outputs'))
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info("="*60)
        logger.info("ProductionTrainer åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"  è®¾å¤‡: {self.device}")
        logger.info(f"  å­¦ä¹ ç‡: {config.get('learning_rate', 2e-5)}")
        logger.info(f"  æ¢¯åº¦ç´¯ç§¯: {self.gradient_accumulation_steps}")
        logger.info(f"  æ··åˆç²¾åº¦: {self.use_amp}")
        logger.info(f"  è¾“å‡ºç›®å½•: {self.output_dir}")
        logger.info("="*60)
    
    def train_epoch(self, dataloader: DataLoader, epoch: int):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        accumulation_step = 0
        
        pbar = tqdm(dataloader, desc=f'Epoch {epoch}')
        
        for batch_idx, batch in enumerate(pbar):
            try:
                # ç§»åŠ¨æ•°æ®åˆ°GPU
                batch = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v 
                        for k, v in batch.items()}
                
                # å‰å‘ä¼ æ’­ï¼ˆæ··åˆç²¾åº¦ï¼‰
                if self.use_amp:
                    with torch.cuda.amp.autocast():
                        outputs = self.model(**batch)
                        loss = outputs.get('loss', outputs.get('logits'))
                        if not isinstance(loss, torch.Tensor):
                            # å¦‚æœæ²¡æœ‰lossï¼Œè®¡ç®—ç®€å•çš„è¯­è¨€æ¨¡å‹loss
                            logits = outputs['logits']
                            labels = batch.get('labels', batch.get('input_ids'))
                            loss = nn.functional.cross_entropy(
                                logits.view(-1, logits.size(-1)),
                                labels.view(-1),
                                ignore_index=-100
                            )
                        loss = loss / self.gradient_accumulation_steps
                else:
                    outputs = self.model(**batch)
                    loss = outputs.get('loss', outputs.get('logits'))
                    if not isinstance(loss, torch.Tensor):
                        logits = outputs['logits']
                        labels = batch.get('labels', batch.get('input_ids'))
                        loss = nn.functional.cross_entropy(
                            logits.view(-1, logits.size(-1)),
                            labels.view(-1),
                            ignore_index=-100
                        )
                    loss = loss / self.gradient_accumulation_steps
                
                # åå‘ä¼ æ’­
                if self.use_amp:
                    self.scaler.scale(loss).backward()
                else:
                    loss.backward()
                
                accumulation_step += 1
                
                # æ¢¯åº¦ç´¯ç§¯åæ›´æ–°
                if accumulation_step % self.gradient_accumulation_steps == 0:
                    # æ¢¯åº¦è£å‰ª
                    if self.use_amp:
                        self.scaler.unscale_(self.optimizer)
                    
                    grad_norm = torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(), 
                        max_norm=1.0
                    )
                    
                    # æ£€æŸ¥æ¢¯åº¦å¼‚å¸¸
                    if self._check_gradients(grad_norm):
                        # ä¼˜åŒ–å™¨æ­¥éª¤
                        if self.use_amp:
                            self.scaler.step(self.optimizer)
                            self.scaler.update()
                        else:
                            self.optimizer.step()
                        
                        self.scheduler.step()
                        self.global_step += 1
                    else:
                        logger.warning(f"è·³è¿‡step {self.global_step}: æ¢¯åº¦å¼‚å¸¸ (norm={grad_norm:.2f})")
                    
                    self.optimizer.zero_grad()
                
                # è®°å½•
                current_loss = loss.item() * self.gradient_accumulation_steps
                total_loss += current_loss
                avg_loss = total_loss / (batch_idx + 1)
                
                # æ›´æ–°è¿›åº¦æ¡
                pbar.set_postfix({
                    'loss': f'{avg_loss:.4f}',
                    'lr': f'{self.scheduler.get_last_lr()[0]:.2e}',
                    'step': self.global_step
                })
                
                # Losså¼‚å¸¸æ£€æµ‹
                if self._detect_loss_spike(current_loss):
                    logger.warning(f"âš ï¸ Lossçªå¢æ£€æµ‹: step {self.global_step}, loss={current_loss:.4f}")
                
                # å®šæœŸä¿å­˜
                if self.global_step % self.config.get('save_steps', 1000) == 0:
                    self.save_checkpoint(epoch, avg_loss)
                
            except Exception as e:
                logger.error(f"è®­ç»ƒæ­¥éª¤ {batch_idx} å‡ºé”™: {e}")
                continue
        
        return total_loss / len(dataloader)
    
    def _check_gradients(self, grad_norm: float) -> bool:
        """æ£€æŸ¥æ¢¯åº¦æ˜¯å¦æ­£å¸¸"""
        # æ£€æŸ¥NaNæˆ–è¿‡å¤§çš„æ¢¯åº¦
        if torch.isnan(torch.tensor(grad_norm)) or grad_norm > 100.0:
            return False
        return True
    
    def _detect_loss_spike(self, current_loss: float) -> bool:
        """æ£€æµ‹Lossçªç„¶é£™å‡"""
        self.loss_history.append(current_loss)
        
        # ä¿æŒæœ€è¿‘100ä¸ªloss
        if len(self.loss_history) > 100:
            self.loss_history.pop(0)
        
        if len(self.loss_history) < 10:
            return False
        
        recent_avg = np.mean(self.loss_history[-10:])
        
        # å¦‚æœå½“å‰lossæ˜¯æœ€è¿‘å¹³å‡å€¼çš„2å€ä»¥ä¸Š
        if current_loss > recent_avg * 2.0 and recent_avg > 0:
            return True
        
        return False
    
    def save_checkpoint(self, epoch: int, loss: float):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint_dir = self.output_dir / 'checkpoints'
        checkpoint_dir.mkdir(exist_ok=True)
        
        checkpoint = {
            'epoch': epoch,
            'global_step': self.global_step,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'loss': loss,
            'config': self.config,
            'loss_history': self.loss_history
        }
        
        # ä¿å­˜å½“å‰checkpoint
        checkpoint_path = checkpoint_dir / f'checkpoint-step{self.global_step}.pt'
        torch.save(checkpoint, checkpoint_path)
        logger.info(f"ğŸ’¾ Checkpointä¿å­˜: {checkpoint_path}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if loss < self.best_loss:
            self.best_loss = loss
            best_path = self.output_dir / 'best_model.pt'
            torch.save(checkpoint, best_path)
            logger.info(f"ğŸ† æœ€ä½³æ¨¡å‹æ›´æ–°: loss={loss:.4f}")
            
            # ä¿å­˜è®­ç»ƒæ—¥å¿—
            self._save_training_log(epoch, loss)
    
    def _save_training_log(self, epoch: int, loss: float):
        """ä¿å­˜è®­ç»ƒæ—¥å¿—"""
        log_path = self.output_dir / 'training_log.json'
        
        log_entry = {
            'epoch': epoch,
            'global_step': self.global_step,
            'loss': loss,
            'best_loss': self.best_loss,
            'learning_rate': self.scheduler.get_last_lr()[0]
        }
        
        # è¿½åŠ åˆ°æ—¥å¿—æ–‡ä»¶
        logs = []
        if log_path.exists():
            with open(log_path, 'r') as f:
                logs = json.load(f)
        
        logs.append(log_entry)
        
        with open(log_path, 'w') as f:
            json.dump(logs, f, indent=2)
    
    def load_checkpoint(self, checkpoint_path: str):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        self.global_step = checkpoint['global_step']
        self.best_loss = checkpoint['loss']
        self.loss_history = checkpoint.get('loss_history', [])
        
        logger.info(f"âœ… ä»checkpointæ¢å¤: {checkpoint_path}")
        logger.info(f"   Epoch: {checkpoint['epoch']}, Step: {self.global_step}, Loss: {checkpoint['loss']:.4f}")
        
        return checkpoint['epoch']


class TrainingConfig:
    """è®­ç»ƒé…ç½®ç±»"""
    
    @staticmethod
    def get_default_config():
        """è·å–é»˜è®¤é…ç½®"""
        return {
            # ä¼˜åŒ–å™¨
            'learning_rate': 2e-5,
            'weight_decay': 0.01,
            'warmup_steps': 500,
            
            # è®­ç»ƒ
            'num_epochs': 3,
            'batch_size': 2,
            'gradient_accumulation': 8,
            'use_amp': True,
            
            # ä¿å­˜å’Œæ—¥å¿—
            'output_dir': 'outputs',
            'save_steps': 500,
            'logging_steps': 10,
            
            # æ•°æ®
            'max_length': 512,
            'num_workers': 4,
        }
    
    @staticmethod
    def from_yaml(yaml_path: str):
        """ä»YAMLæ–‡ä»¶åŠ è½½é…ç½®"""
        import yaml
        with open(yaml_path, 'r') as f:
            config = yaml.safe_load(f)
        return config


if __name__ == '__main__':
    # æµ‹è¯•è®­ç»ƒå™¨åˆ›å»º
    print("æµ‹è¯•ProductionTrainer...")
    
    config = TrainingConfig.get_default_config()
    config['total_steps'] = 1000
    
    # åˆ›å»ºç®€å•æ¨¡å‹æµ‹è¯•
    class DummyModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.linear = nn.Linear(10, 10)
        
        def forward(self, input_ids, **kwargs):
            x = torch.randn(2, 10)
            logits = self.linear(x)
            return {'logits': logits}
    
    model = DummyModel()
    trainer = ProductionTrainer(model, config)
    
    print("âœ… ProductionTraineråˆ›å»ºæˆåŠŸï¼")
