import sys
import torch
import platform

def verify_environment():
    print("=" * 60)
    print("ğŸ” å¤šæ¨¡æ€æ¨¡å‹è®­ç»ƒç¯å¢ƒéªŒè¯")
    print("=" * 60)
    
    print(f"\nğŸ“Œ ç³»ç»Ÿä¿¡æ¯:")
    print(f"  æ“ä½œç³»ç»Ÿ: {platform.system()} {platform.release()}")
    print(f"  Pythonç‰ˆæœ¬: {sys.version.split()[0]}")
    print(f"  å¤„ç†å™¨: {platform.processor()}")
    
    print(f"\nğŸ“¦ æ ¸å¿ƒä¾èµ–:")
    try:
        print(f"  PyTorch: {torch.__version__}")
    except:
        print("  âŒ PyTorchæœªå®‰è£…")
        return False
    
    try:
        import transformers
        print(f"  Transformers: {transformers.__version__}")
    except:
        print("  âš ï¸  Transformersæœªå®‰è£…ï¼ˆæ¨èå®‰è£…ï¼‰")
    
    try:
        import accelerate
        print(f"  Accelerate: {accelerate.__version__}")
    except:
        print("  âš ï¸  Accelerateæœªå®‰è£…ï¼ˆæ¨èå®‰è£…ï¼‰")
    
    print(f"\nğŸ® GPUä¿¡æ¯:")
    cuda_available = torch.cuda.is_available()
    print(f"  CUDAå¯ç”¨: {'âœ… æ˜¯' if cuda_available else 'âŒ å¦'}")
    
    if cuda_available:
        print(f"  CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"  cuDNNç‰ˆæœ¬: {torch.backends.cudnn.version()}")
        gpu_count = torch.cuda.device_count()
        print(f"  GPUæ•°é‡: {gpu_count}")
        
        for i in range(gpu_count):
            props = torch.cuda.get_device_properties(i)
            print(f"\n  GPU {i}: {torch.cuda.get_device_name(i)}")
            print(f"    æ€»æ˜¾å­˜: {props.total_memory / 1024**3:.2f} GB")
            print(f"    è®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}")
            
            if torch.cuda.is_available():
                torch.cuda.set_device(i)
                allocated = torch.cuda.memory_allocated(i) / 1024**3
                reserved = torch.cuda.memory_reserved(i) / 1024**3
                print(f"    å·²åˆ†é…æ˜¾å­˜: {allocated:.2f} GB")
                print(f"    å·²ä¿ç•™æ˜¾å­˜: {reserved:.2f} GB")
    else:
        print("  âš ï¸  æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒï¼ˆé€Ÿåº¦ä¼šå¾ˆæ…¢ï¼‰")
    
    print(f"\nğŸ§ª å¿«é€ŸåŠŸèƒ½æµ‹è¯•:")
    try:
        print("  æµ‹è¯•å¼ é‡åˆ›å»º...", end=" ")
        x = torch.randn(100, 100)
        print("âœ…")
        
        if cuda_available:
            print("  æµ‹è¯•GPUå¼ é‡...", end=" ")
            x_gpu = torch.randn(100, 100).cuda()
            print("âœ…")
            
            print("  æµ‹è¯•GPUè®¡ç®—...", end=" ")
            y = torch.matmul(x_gpu, x_gpu)
            print("âœ…")
    except Exception as e:
        print(f"âŒ {e}")
        return False
    
    print("\n" + "=" * 60)
    print("âœ… ç¯å¢ƒéªŒè¯å®Œæˆï¼")
    
    if not cuda_available:
        print("\nâš ï¸  è­¦å‘Š: æœªæ£€æµ‹åˆ°GPU")
        print("å»ºè®®:")
        print("  1. æ£€æŸ¥NVIDIAé©±åŠ¨æ˜¯å¦æ­£ç¡®å®‰è£…")
        print("  2. ç¡®è®¤PyTorchå®‰è£…çš„CUDAç‰ˆæœ¬ä¸ç³»ç»ŸåŒ¹é…")
        print("  3. è¿è¡Œ: nvidia-smi æŸ¥çœ‹GPUçŠ¶æ€")
    
    print("\nğŸ“š ä¸‹ä¸€æ­¥:")
    print("  1. è¿è¡Œ: python scripts/test_model_architecture.py")
    print("  2. è¿è¡Œ: python scripts/create_sample_data.py")
    print("  3. è¿è¡Œ: python scripts/test_data_pipeline.py")
    print("=" * 60)
    
    return True


if __name__ == "__main__":
    success = verify_environment()
    sys.exit(0 if success else 1)
