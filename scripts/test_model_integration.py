"""æµ‹è¯•æ”¹è¿›ç»„ä»¶é›†æˆåˆ°ä¸»æ¨¡å‹"""
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent / 'src'))

import torch
from model_architecture import MultimodalReasoningModel

def test_model_with_improved_components():
    """æµ‹è¯•ä½¿ç”¨æ”¹è¿›ç»„ä»¶çš„æ¨¡å‹"""
    print("="*60)
    print("æµ‹è¯•æ”¹è¿›ç»„ä»¶é›†æˆ")
    print("="*60)
    
    # é…ç½® - ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹è¿›è¡Œæµ‹è¯•
    config = {
        'base_model': 'Qwen/Qwen2-0.5B',  # ä½¿ç”¨å°æ¨¡å‹å¿«é€Ÿæµ‹è¯•
        'img_size': 384,
        'patch_size': 14,
        'vision_embed_dim': 1152,
        'vision_depth': 27,
        'vision_heads': 16,
        'use_flash_attn': True,
        'projector_type': 'token_pooling',
        'pooling_kernel': 2,
        'enable_audio': False,
        'enable_cot': True,
        'enable_verification': True
    }
    
    try:
        print("\n[1/4] åˆ›å»ºæ¨¡å‹...")
        model = MultimodalReasoningModel(config)
        print("âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        
        # ç»Ÿè®¡å‚æ•°
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        print(f"\n[2/4] æ¨¡å‹å‚æ•°ç»Ÿè®¡:")
        print(f"  æ€»å‚æ•°: {total_params / 1e9:.2f}B")
        print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params / 1e9:.2f}B")
        print(f"  å¯è®­ç»ƒæ¯”ä¾‹: {trainable_params / total_params * 100:.1f}%")
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        print(f"\n[3/4] æµ‹è¯•å‰å‘ä¼ æ’­...")
        batch_size = 2
        seq_len = 32
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®ï¼ˆè½¬æ¢ä¸ºbfloat16ï¼‰
        input_ids = torch.randint(0, 1000, (batch_size, seq_len))
        images = torch.randn(batch_size, 3, 384, 384).to(torch.bfloat16)
        labels = torch.randint(0, 1000, (batch_size, seq_len))
        
        # å‰å‘ä¼ æ’­ï¼ˆä¸ä½¿ç”¨labelsï¼‰
        with torch.no_grad():
            outputs = model(
                input_ids=input_ids,
                images=images
            )
        
        print("âœ… å‰å‘ä¼ æ’­æˆåŠŸ")
        print(f"  è¾“å‡ºlogits shape: {outputs['logits'].shape}")
        print(f"  Hidden states shape: {outputs['hidden_states'].shape}")
        
        # æµ‹è¯•ç”Ÿæˆ
        print(f"\n[4/4] æµ‹è¯•ç”ŸæˆåŠŸèƒ½...")
        with torch.no_grad():
            generated = model.generate(
                input_ids=input_ids[:1],
                images=images[:1],
                max_length=50,
                temperature=0.7
            )
        
        print("âœ… ç”ŸæˆåŠŸèƒ½æ­£å¸¸")
        print(f"  ç”Ÿæˆçš„tokenæ•°: {generated.shape[1]}")
        
        print("\n" + "="*60)
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ”¹è¿›ç»„ä»¶é›†æˆæˆåŠŸï¼")
        print("="*60)
        
        return True
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == '__main__':
    success = test_model_with_improved_components()
    sys.exit(0 if success else 1)
