# å¤šæ¨¡æ€æ™ºèƒ½æ¨¡å‹è®­ç»ƒæŒ‡å— ğŸš€

æœ¬é¡¹ç›®æä¾›äº†ä¸€å¥—å®Œæ•´çš„å¤šæ¨¡æ€æ™ºèƒ½æ¨¡å‹è®­ç»ƒæ¡†æ¶ï¼Œæ”¯æŒè§†è§‰ã€éŸ³é¢‘ã€æ–‡æœ¬ç­‰å¤šç§æ¨¡æ€çš„èåˆå¤„ç†ï¼Œå¹¶å…·å¤‡å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚

## ğŸ“‹ ç›®å½•

- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [é¡¹ç›®ç»“æ„](#é¡¹ç›®ç»“æ„)
- [ç¯å¢ƒé…ç½®](#ç¯å¢ƒé…ç½®)
- [æ•°æ®å‡†å¤‡](#æ•°æ®å‡†å¤‡)
- [æ¨¡å‹è®­ç»ƒ](#æ¨¡å‹è®­ç»ƒ)
- [æ¨¡å‹è¯„ä¼°](#æ¨¡å‹è¯„ä¼°)
- [æ¨¡å‹éƒ¨ç½²](#æ¨¡å‹éƒ¨ç½²)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå®‰è£…

```bash
# å…‹éš†é¡¹ç›®
git clone https://github.com/your-org/OpenMind.git
cd OpenMind

# è¿è¡Œç¯å¢ƒé…ç½®è„šæœ¬
bash scripts/setup_environment.sh

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source venv/bin/activate  # Linux/Mac
# æˆ–
.\venv\Scripts\activate   # Windows
```

### 2. å‡†å¤‡æ•°æ®

```bash
# åˆ›å»ºæ•°æ®ç›®å½•
mkdir -p data/{pretrain,sft,eval}

# ä¸‹è½½ç¤ºä¾‹æ•°æ®ï¼ˆéœ€è¦é…ç½®æ•°æ®æºï¼‰
python scripts/download_datasets.py --config configs/data_config.yaml
```

### 3. å¼€å§‹è®­ç»ƒ

```bash
# é˜¶æ®µ1: å¤šæ¨¡æ€é¢„è®­ç»ƒ
python src/train_multimodal.py \
    --config configs/training_config.yaml \
    --stage pretrain \
    --num_epochs 10

# é˜¶æ®µ2: æŒ‡ä»¤å¾®è°ƒ
python src/train_multimodal.py \
    --config configs/training_config.yaml \
    --stage sft \
    --num_epochs 5
```

---

## ğŸ“ é¡¹ç›®ç»“æ„

```
OpenMind/
â”œâ”€â”€ configs/                      # é…ç½®æ–‡ä»¶ç›®å½•
â”‚   â”œâ”€â”€ training_config.yaml      # è®­ç»ƒé…ç½®
â”‚   â”œâ”€â”€ model_config.yaml         # æ¨¡å‹é…ç½®
â”‚   â””â”€â”€ data_config.yaml          # æ•°æ®é…ç½®
â”‚
â”œâ”€â”€ data/                         # æ•°æ®ç›®å½•
â”‚   â”œâ”€â”€ pretrain/                 # é¢„è®­ç»ƒæ•°æ®
â”‚   â”œâ”€â”€ sft/                      # æŒ‡ä»¤å¾®è°ƒæ•°æ®
â”‚   â””â”€â”€ eval/                     # è¯„ä¼°æ•°æ®
â”‚
â”œâ”€â”€ src/                          # æºä»£ç ç›®å½•
â”‚   â”œâ”€â”€ model_architecture.py     # æ¨¡å‹æ¶æ„
â”‚   â”œâ”€â”€ data_pipeline.py          # æ•°æ®å¤„ç†
â”‚   â”œâ”€â”€ train_multimodal.py       # è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ evaluate_model.py         # è¯„ä¼°è„šæœ¬
â”‚   â””â”€â”€ inference.py              # æ¨ç†è„šæœ¬
â”‚
â”œâ”€â”€ scripts/                      # å·¥å…·è„šæœ¬
â”‚   â”œâ”€â”€ setup_environment.sh      # ç¯å¢ƒé…ç½®
â”‚   â”œâ”€â”€ download_datasets.py      # æ•°æ®ä¸‹è½½
â”‚   â””â”€â”€ convert_checkpoint.py     # æ¨¡å‹è½¬æ¢
â”‚
â”œâ”€â”€ checkpoints/                  # æ¨¡å‹æ£€æŸ¥ç‚¹
â”œâ”€â”€ logs/                         # è®­ç»ƒæ—¥å¿—
â”œâ”€â”€ outputs/                      # è¾“å‡ºç»“æœ
â”‚
â”œâ”€â”€ docs/                         # æ–‡æ¡£ç›®å½•
â”‚   â””â”€â”€ MULTIMODAL_MODEL_TRAINING_PLAN.md
â”‚
â””â”€â”€ README_TRAINING.md            # æœ¬æ–‡ä»¶
```

---

## âš™ï¸ ç¯å¢ƒé…ç½®

### ç¡¬ä»¶è¦æ±‚

#### æœ€ä½é…ç½®
- GPU: 1x RTX 4090 (24GB) æˆ– A6000 (48GB)
- RAM: 32GB
- å­˜å‚¨: 500GB SSD

#### æ¨èé…ç½®
- GPU: 4x A100 (80GB) æˆ– 8x H100 (80GB)
- RAM: 256GB
- å­˜å‚¨: 2TB NVMe SSD

#### äº‘æœåŠ¡æ¨è
- AWS: `p4d.24xlarge` (8x A100 80GB)
- é˜¿é‡Œäº‘: `ecs.gn7i-c64g1.24xlarge` (8x A100 80GB)
- è…¾è®¯äº‘: `GT4.20XLARGE464` (8x A100 80GB)

### è½¯ä»¶è¦æ±‚

- Python 3.9+
- CUDA 11.8+ / 12.1+
- PyTorch 2.1+
- Transformers 4.36+

---

## ğŸ“¦ æ•°æ®å‡†å¤‡

### æ•°æ®æ ¼å¼

#### é¢„è®­ç»ƒæ•°æ®æ ¼å¼ (JSONL)

```json
{
  "text": "å›¾ç‰‡æè¿°æ–‡æœ¬",
  "image": "/path/to/image.jpg",
  "metadata": {
    "source": "laion",
    "quality_score": 0.85
  }
}
```

#### æŒ‡ä»¤å¾®è°ƒæ•°æ®æ ¼å¼

```json
{
  "conversations": [
    {"from": "user", "value": "è¿™å¼ å›¾ç‰‡ä¸­æœ‰ä»€ä¹ˆï¼Ÿ"},
    {"from": "assistant", "value": "å›¾ç‰‡ä¸­æ˜¾ç¤ºäº†ä¸€åªå¯çˆ±çš„çŒ«å’ª..."}
  ],
  "image": "/path/to/image.jpg"
}
```

### æ¨èæ•°æ®é›†

#### å›¾åƒ-æ–‡æœ¬æ•°æ®
- LAION-5B: 50äº¿å›¾æ–‡å¯¹
- CC12M: 1200ä¸‡é«˜è´¨é‡å›¾æ–‡å¯¹
- COCO: 33ä¸‡æ ‡æ³¨å›¾åƒ
- Visual Genome: 10.8ä¸‡å›¾åƒå…³ç³»æ•°æ®

#### è§†é¢‘æ•°æ®
- WebVid: 1000ä¸‡è§†é¢‘ç‰‡æ®µ
- HowTo100M: 1.36äº¿è§†é¢‘ç‰‡æ®µ

#### æ–‡æ¡£ç†è§£
- DocVQA: 5ä¸‡æ–‡æ¡£é—®ç­”
- ChartQA: 3.2ä¸‡å›¾è¡¨é—®ç­”
- TextVQA: 4.5ä¸‡æ–‡æœ¬é—®ç­”

#### æ¨ç†æ•°æ®
- GSM8K: 8000+æ•°å­¦é—®é¢˜
- MATH: 12000é«˜çº§æ•°å­¦é¢˜
- HumanEval: 164ä»£ç é—®é¢˜

### æ•°æ®ä¸‹è½½è„šæœ¬

```bash
# ä¸‹è½½LAIONæ•°æ®å­é›†
python scripts/download_datasets.py \
    --dataset laion \
    --subset 2B \
    --output data/pretrain/laion

# ä¸‹è½½COCOæ•°æ®
python scripts/download_datasets.py \
    --dataset coco \
    --year 2017 \
    --output data/pretrain/coco
```

---

## ğŸ“ æ¨¡å‹è®­ç»ƒ

### ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥

#### é˜¶æ®µ1: å¤šæ¨¡æ€é¢„è®­ç»ƒ (40-60å¤©)

**ç›®æ ‡**: å»ºç«‹è·¨æ¨¡æ€å¯¹é½èƒ½åŠ›

```bash
python src/train_multimodal.py \
    --config configs/training_config.yaml \
    --stage pretrain \
    --num_epochs 10 \
    --batch_size 4 \
    --gradient_accumulation_steps 4 \
    --learning_rate 1e-5 \
    --output_dir checkpoints/pretrain
```

**å…³é”®å‚æ•°**:
- `batch_size`: æ¯GPUæ‰¹æ¬¡å¤§å°
- `gradient_accumulation_steps`: æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
- `learning_rate`: å­¦ä¹ ç‡ (æ¨è 1e-5 åˆ° 5e-5)

#### é˜¶æ®µ2: æŒ‡ä»¤å¾®è°ƒ (15-30å¤©)

**ç›®æ ‡**: æå‡ä»»åŠ¡éµå¾ªå’Œå¯¹è¯èƒ½åŠ›

```bash
python src/train_multimodal.py \
    --config configs/training_config.yaml \
    --stage sft \
    --num_epochs 5 \
    --batch_size 2 \
    --gradient_accumulation_steps 8 \
    --learning_rate 5e-6 \
    --output_dir checkpoints/sft
```

#### é˜¶æ®µ3: å¼ºåŒ–å­¦ä¹  (20-40å¤©ï¼Œå¯é€‰)

**ç›®æ ‡**: æ¿€å‘æ¨ç†èƒ½åŠ›ï¼Œå¯¹é½äººç±»åå¥½

```bash
python src/train_rlhf.py \
    --config configs/rl_config.yaml \
    --base_model checkpoints/sft/best_model \
    --reward_model checkpoints/reward_model \
    --num_epochs 3 \
    --output_dir checkpoints/rl
```

### åˆ†å¸ƒå¼è®­ç»ƒ

#### ä½¿ç”¨ DeepSpeed

```bash
deepspeed --num_gpus=8 src/train_multimodal.py \
    --config configs/training_config.yaml \
    --deepspeed configs/deepspeed_config.json
```

#### ä½¿ç”¨ Accelerate

```bash
accelerate launch --multi_gpu --num_processes=8 \
    src/train_multimodal.py \
    --config configs/training_config.yaml
```

### è®­ç»ƒç›‘æ§

#### ä½¿ç”¨ Wandb

```bash
# åœ¨é…ç½®æ–‡ä»¶ä¸­å¯ç”¨wandb
use_wandb: true
project_name: "multimodal-training"

# æˆ–å‘½ä»¤è¡Œå¯ç”¨
python src/train_multimodal.py \
    --config configs/training_config.yaml \
    --use_wandb \
    --wandb_project "multimodal-training"
```

#### ä½¿ç”¨ TensorBoard

```bash
# å¯åŠ¨TensorBoard
tensorboard --logdir logs/

# æµè§ˆå™¨è®¿é—®: http://localhost:6006
```

---

## ğŸ“Š æ¨¡å‹è¯„ä¼°

### è¿è¡Œè¯„ä¼°

```bash
python src/evaluate_model.py \
    --model_config configs/model_config.yaml \
    --checkpoint checkpoints/sft/checkpoint-epoch-5/model.pt \
    --eval_config configs/eval_config.yaml
```

### è¯„ä¼°æŒ‡æ ‡

#### è§†è§‰ç†è§£
- **VQAå‡†ç¡®ç‡**: Visual Question Answering
- **COCO CIDEr**: å›¾åƒæè¿°è´¨é‡
- **TextVQAå‡†ç¡®ç‡**: æ–‡æœ¬è¯†åˆ«é—®ç­”

#### æ¨ç†èƒ½åŠ›
- **GSM8Kå‡†ç¡®ç‡**: æ•°å­¦æ¨ç†
- **MATHå‡†ç¡®ç‡**: é«˜çº§æ•°å­¦
- **HumanEval Pass@1**: ä»£ç ç”Ÿæˆ

#### è¯­è¨€èƒ½åŠ›
- **Perplexity**: å›°æƒ‘åº¦
- **BLEU/ROUGE**: æ–‡æœ¬ç”Ÿæˆè´¨é‡

### åŸºå‡†æµ‹è¯•

```bash
# è¿è¡Œå®Œæ•´åŸºå‡†æµ‹è¯•
bash scripts/run_benchmarks.sh \
    --model checkpoints/sft/best_model \
    --output results/benchmarks.json
```

---

## ğŸš€ æ¨¡å‹éƒ¨ç½²

### æœ¬åœ°æ¨ç†

```python
from src.model_architecture import create_model
import torch

# åŠ è½½æ¨¡å‹
config = {
    'base_model': 'Qwen/Qwen2-7B',
    'vision_model': 'openai/clip-vit-large-patch14',
    # ...å…¶ä»–é…ç½®
}
model = create_model(config)
model.load_state_dict(torch.load('checkpoints/best_model.pt'))
model.eval()

# æ¨ç†
input_ids = tokenizer("ä½ å¥½ï¼Œè¯·æè¿°è¿™å¼ å›¾ç‰‡", return_tensors='pt').input_ids
images = load_image("test.jpg")

with torch.no_grad():
    output = model.generate(
        input_ids=input_ids,
        images=images,
        max_length=512
    )

print(tokenizer.decode(output[0]))
```

### API æœåŠ¡éƒ¨ç½²

```bash
# ä½¿ç”¨ FastAPI éƒ¨ç½²
python scripts/serve_api.py \
    --model checkpoints/best_model \
    --port 8000 \
    --workers 4
```

### æ¨¡å‹é‡åŒ–

```bash
# INT8é‡åŒ–
python scripts/quantize_model.py \
    --model checkpoints/best_model \
    --bits 8 \
    --output checkpoints/quantized_int8

# INT4é‡åŒ– (éœ€è¦bitsandbytes)
python scripts/quantize_model.py \
    --model checkpoints/best_model \
    --bits 4 \
    --output checkpoints/quantized_int4
```

---

## â“ å¸¸è§é—®é¢˜

### Q1: æ˜¾å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ

**è§£å†³æ–¹æ¡ˆ**:
1. å‡å° `batch_size`ï¼Œå¢å¤§ `gradient_accumulation_steps`
2. å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹: `use_gradient_checkpointing: true`
3. ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ: `mixed_precision: "bf16"`
4. ä½¿ç”¨ LoRA å¾®è°ƒ: `use_lora: true`
5. å†»ç»“è§†è§‰ç¼–ç å™¨: `freeze_vision: true`

### Q2: è®­ç»ƒé€Ÿåº¦å¤ªæ…¢ï¼Ÿ

**è§£å†³æ–¹æ¡ˆ**:
1. ä½¿ç”¨ Flash Attention: `use_flash_attention: true`
2. å¯ç”¨ DeepSpeed ZeROä¼˜åŒ–
3. å¢åŠ  `num_workers` æé«˜æ•°æ®åŠ è½½é€Ÿåº¦
4. ä½¿ç”¨ WebDataset æ ¼å¼çš„æ•°æ®
5. å¯ç”¨ç¼–è¯‘ä¼˜åŒ–: `torch.compile(model)`

### Q3: å¦‚ä½•ç»§ç»­è®­ç»ƒï¼Ÿ

```bash
python src/train_multimodal.py \
    --config configs/training_config.yaml \
    --resume_from_checkpoint checkpoints/checkpoint-epoch-5
```

### Q4: å¦‚ä½•åªå¾®è°ƒéƒ¨åˆ†å‚æ•°ï¼Ÿ

```yaml
# åœ¨é…ç½®æ–‡ä»¶ä¸­å¯ç”¨LoRA
use_lora: true
lora_config:
  r: 8
  lora_alpha: 16
  target_modules: ["q_proj", "v_proj", "k_proj", "o_proj"]
```

### Q5: æ¨ç†æ—¶å¦‚ä½•åˆ‡æ¢æ€è€ƒæ¨¡å¼ï¼Ÿ

```python
# å¯ç”¨æ¨ç†æ¨¡å¼
output = model.generate(
    input_ids=input_ids,
    images=images,
    use_reasoning=True,  # å¯ç”¨æ€ç»´é“¾æ¨ç†
    max_length=1024
)
```

---

## ğŸ“š å‚è€ƒèµ„æº

### è®ºæ–‡
- [DeepSeek-V3 Technical Report](https://arxiv.org/pdf/2412.19437)
- [Qwen3 Technical Report](https://github.com/QwenLM/Qwen3)
- [LLaVA: Visual Instruction Tuning](https://arxiv.org/abs/2304.08485)

### ä»£ç åº“
- [DeepSeek-AI](https://github.com/deepseek-ai)
- [QwenLM](https://github.com/QwenLM)
- [HuggingFace Transformers](https://github.com/huggingface/transformers)

### ç¤¾åŒº
- [HuggingFace Forums](https://discuss.huggingface.co/)
- [Discord](https://discord.gg/huggingface)

---

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ Apache 2.0 è®¸å¯è¯ã€‚è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶ã€‚

---

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

---

**æœ€åæ›´æ–°**: 2025å¹´1æœˆ  
**ç»´æŠ¤è€…**: OpenMindå›¢é˜Ÿ  
**è”ç³»æ–¹å¼**: [å¾…è¡¥å……]
