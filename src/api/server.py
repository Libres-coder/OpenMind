import torch
import asyncio
from fastapi import FastAPI, HTTPException, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Optional, List, Dict, Any
import base64
import io
from PIL import Image
import numpy as np
# å¯¼å…¥Agent
import sys
sys.path.insert(0, '/root/autodl-tmp/OpenMind')
from src.core import OpenMindAgent, AgentConfig
# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="OpenMind API",
    description="å¤šæ¨¡æ€æ™ºèƒ½Agent APIæœåŠ¡",
    version="1.0.0"
)
# CORSé…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
# å…¨å±€Agentå®ä¾‹
agent = None
device = None
# è¯·æ±‚æ¨¡å‹
class ChatRequest(BaseModel):
    message: str
    image_base64: Optional[str] = None
    use_reasoning: bool = True
    use_evolution: bool = True
class ChatResponse(BaseModel):
    status: str
    mode: str
    output_shape: List[int]
    reasoning_steps: Optional[int] = None
    evolution_score: Optional[float] = None
    memory_context_length: Optional[int] = None
class AnalyzeImageRequest(BaseModel):
    image_base64: str
    task: str = "classify"  # classify, caption, encode
class StatsResponse(BaseModel):
    total_parameters: str
    trainable_parameters: str
    components: Dict[str, str]
    memory_stats: Dict[str, Any]
    evolution_stats: Dict[str, Any]
@app.on_event("startup")
async def startup_event():
    """å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹"""
    global agent, device
    
    print("ğŸš€ æ­£åœ¨åŠ è½½OpenMind Agent...")
    
    # æ£€æµ‹è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ“ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºé…ç½®
    config = AgentConfig(
        hidden_size=768,
        max_cot_steps=5,
        img_size=224,
        vision_layers=6,
        fusion_layers=4
    )
    
    # åˆ›å»ºAgent
    agent = OpenMindAgent(config)
    agent = agent.to(device)
    agent.eval()
    
    print(f"âœ… AgentåŠ è½½å®Œæˆ! å‚æ•°é‡: {sum(p.numel() for p in agent.parameters())/1e6:.2f}M")
@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "service": "OpenMind API",
        "version": "1.0.0",
        "status": "running",
        "endpoints": ["/chat", "/analyze", "/stats", "/health"]
    }
@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {
        "status": "healthy",
        "agent_loaded": agent is not None,
        "device": str(device)
    }
@app.get("/stats", response_model=StatsResponse)
async def get_stats():
    """è·å–Agentç»Ÿè®¡ä¿¡æ¯"""
    if agent is None:
        raise HTTPException(status_code=503, detail="AgentæœªåŠ è½½")
    
    stats = agent.get_stats()
    return StatsResponse(**stats)
@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """å¯¹è¯æ¥å£"""
    if agent is None:
        raise HTTPException(status_code=503, detail="AgentæœªåŠ è½½")
    
    try:
        # åˆ›å»ºæ–‡æœ¬åµŒå…¥ (æ¨¡æ‹Ÿï¼Œå®é™…åº”è¯¥ç”¨tokenizer)
        text_emb = torch.randn(1, 768).to(device)
        
        # å¤„ç†å›¾åƒ
        image_tensor = None
        if request.image_base64:
            # è§£ç base64å›¾åƒ
            image_data = base64.b64decode(request.image_base64)
            image = Image.open(io.BytesIO(image_data)).convert('RGB')
            image = image.resize((224, 224))
            image_np = np.array(image) / 255.0
            image_tensor = torch.from_numpy(image_np).permute(2, 0, 1).unsqueeze(0).float().to(device)
        
        # è°ƒç”¨Agent
        with torch.no_grad():
            if image_tensor is not None:
                result = agent.chat(request.message, text_emb, image_tensor)
                mode = "multimodal"
            else:
                result = agent.chat(request.message, text_emb)
                mode = "text"
        
        # æ„å»ºå“åº”
        response = ChatResponse(
            status="success",
            mode=mode,
            output_shape=list(result['output'].shape),
            reasoning_steps=result.get('reasoning', {}).get('chain_of_thought', {}).get('num_steps'),
            evolution_score=result.get('evolution', {}).get('evaluation', {}).get('overall_score', torch.tensor(0)).mean().item() if 'evolution' in result else None,
            memory_context_length=len(result.get('memory_context', ''))
        )
        
        return response
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
@app.post("/analyze")
async def analyze_image(request: AnalyzeImageRequest):
    """å›¾åƒåˆ†ææ¥å£"""
    if agent is None:
        raise HTTPException(status_code=503, detail="AgentæœªåŠ è½½")
    
    try:
        # è§£ç å›¾åƒ
        image_data = base64.b64decode(request.image_base64)
        image = Image.open(io.BytesIO(image_data)).convert('RGB')
        image = image.resize((224, 224))
        image_np = np.array(image) / 255.0
        image_tensor = torch.from_numpy(image_np).permute(2, 0, 1).unsqueeze(0).float().to(device)
        
        # è°ƒç”¨è§†è§‰ç³»ç»Ÿ
        with torch.no_grad():
            result = agent.vision(image_tensor, task=request.task)
        
        response = {
            "status": "success",
            "task": request.task
        }
        
        if request.task == "classify":
            logits = result.get('logits')
            if logits is not None:
                top_k = torch.topk(logits, 5, dim=-1)
                response["top_5_classes"] = top_k.indices[0].tolist()
                response["top_5_scores"] = top_k.values[0].tolist()
        elif request.task == "encode":
            response["cls_token_shape"] = list(result.get('vision_cls', torch.zeros(1)).shape)
            
        return response
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
print("âœ… APIæœåŠ¡æ¨¡å—åˆ›å»ºå®Œæˆ")
